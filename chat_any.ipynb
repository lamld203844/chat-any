{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lamld203844/chat-any/blob/main/chat_any.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45zCoVjWP49j"
      },
      "source": [
        "# System flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pwtnTyNdWMV"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q langchain-community\n",
        "!pip install -q llama-index\n",
        "\n",
        "!pip install -q llama-index-embeddings-langchain\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "!pip install -q llama-index-llms-gemini\n",
        "!pip install -q google-generativeai\n",
        "!pip install -q llama-index-readers-web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zbu7Ph0kJmiS"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "# allows nested access to the event loop\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDFzArGBiYOW"
      },
      "source": [
        "## Load file\n",
        "- load website"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjKWlYSriaFk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -------------------------------------------\n",
        "# Load data from a website via Llamaindex Loader\n",
        "#\n",
        "# -------------------------------------------\n",
        "\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from IPython.display import Markdown, display\n",
        "import os\n",
        "\n",
        "docs = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [\"https://cinnamon.is/en/\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xikTNUriVNRX"
      },
      "source": [
        "## Chunking and creating embeddings model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX1aUNUfNxjY",
        "outputId": "67a647a1-8d34-45d2-eb01-d17aa2cff410"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding: [0.05707494914531708, 0.002089159097522497, 0.031196104362607002, -0.025225955992937088, 0.011089162901043892, -0.014548588544130325, 0.03243309631943703, 0.024354225024580956, 0.012902941554784775, 0.04747820645570755]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --------------------------------------------\n",
        "# Chunking and create embeddings\n",
        "# Automatic via llama index VectorStoreIndex\n",
        "# --------------------------------------------\n",
        "from torch import cuda\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "def load_embedding_model(\n",
        "    model_name: str = \"BAAI/bge-large-en-v1.5\",\n",
        "    device: str = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        ") -> HuggingFaceBgeEmbeddings:\n",
        "    model_kwargs = {\"device\": device}\n",
        "    encode_kwargs = {\n",
        "        \"normalize_embeddings\": True\n",
        "    }  # set True to compute cosine similarity\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs,\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "# setting up the embedding model\n",
        "lc_embedding_model = load_embedding_model()\n",
        "embed_model = LangchainEmbedding(lc_embedding_model)\n",
        "\n",
        "### Sanity check embedding model\n",
        "embedding = lc_embedding_model.embed_query('Hello, world')\n",
        "embedding = embedding[:10]\n",
        "print(f'Embedding: {embedding}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "460ec774bda54dd29da57bf313c877e8",
            "4124c7030d9448f6a2f0f312c717d92a",
            "af925b0ca8e84713aa94532af835b69e",
            "641ec30a759f4ccfa6f6834d129b6c4e",
            "80b9c3f5a0134c37b15664cf93d603a3",
            "6b1d02d0b25b4818ba1c7e0860533c86",
            "4cecaec3b11c4915a5a0ca8a0092e1e0",
            "9244cbd583c049c8a1ccad35fe31186d",
            "9467733a47a947bfbff60575465e3706",
            "416aa61e14014266aef39637e8182c78",
            "14f2fc9deb5f412a88fa7694bace7752",
            "9252b45e0ffd472a9df77218e2a3b00b",
            "db5960481a92496a9f384ea565815965",
            "27f17b36e8704cd0b9c6a3965c93af14",
            "07999175b7db48d6a9a5ae59b651428b",
            "afdd8058a1134c0eba7dd218a60ac623",
            "4d9e20e7c0d94fc4aafc7c385dcedf85",
            "7978afa8c91b462d913329881e35a2a7",
            "15d0e6881e8a4134b61ddcfdb3cdd545",
            "1dedae21d4fc4be19c461dc4ffad53b2",
            "9ac65e4a4b3348dcbef179ad494f4927",
            "a39a3b3f845b4267982f56e6b8e87298"
          ]
        },
        "id": "RaVCrosIdy_H",
        "outputId": "8b3fa0a1-0d13-415d-a3d4-031deb26d028"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "460ec774bda54dd29da57bf313c877e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9252b45e0ffd472a9df77218e2a3b00b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ====== Create vector store and upload data ======\n",
        "Settings.embed_model = lc_embedding_model\n",
        "index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
        "# TODO try async index creation for faster emebdding generation & persist it to memory!\n",
        "# index = VectorStoreIndex(docs, use_async=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP1yyMtZVWKx"
      },
      "source": [
        "## Load llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jbbfYZZNVZeF",
        "outputId": "9b80823e-8070-46ed-8dd1-79c96de1af82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, world! I am a large language model, trained by Google.\n"
          ]
        }
      ],
      "source": [
        "# --------------------------\n",
        "# Set up search query engine\n",
        "# --------------------------\n",
        "\n",
        "# setting up the llm\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from google.colab import userdata\n",
        "\n",
        "google_api = userdata.get('GOOGLE_API_KEY')\n",
        "llm = Gemini(model_name=\"models/gemini-pro\", api_key=google_api)\n",
        "\n",
        "# Sanity check llm\n",
        "resp = llm.complete(\"Hello, world\")\n",
        "print(resp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9WsHNoveIlP"
      },
      "source": [
        "## Set up query engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6I02L3QeKer"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ====== Setup a query engine ======\n",
        "Settings.llm = llm\n",
        "query_engine = index.as_query_engine(similarity_top_k=4)\n",
        "\n",
        "# ---------------------------------------\n",
        "# Customise prompt template + augmenting\n",
        "# ---------------------------------------\n",
        "\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "qa_prompt_tmpl_str = (\n",
        "  \"You are a friendly and supportive assistant for question answering information from website\"\n",
        "  \"Answer the question using the following information delimited by triple brackque, incase case you don't know the answer say 'I don't know!':\\n\\n\"\n",
        "  \"```\\n{context_str}\\n```\"\n",
        "  \"Query: {query_str}\\n\"\n",
        "  \"\\nDon't say based on information provided or something like that\"\n",
        ")\n",
        "\n",
        "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "MpgJ3bmieM2G",
        "outputId": "bc2fd6c3-31b1-43c9-cb95-16e51b49e3a2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "This website is about Cinnamon, an AI company that provides deep learning backed AI products.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# ---------- Chatting -----------\n",
        "from IPython.display import Markdown, display\n",
        "response = query_engine.query('What is this website about?')\n",
        "display(Markdown(str(response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVEqUqJcKPdH"
      },
      "source": [
        "# App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sk_aDKJKQk4"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "# allows nested access to the event loop\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "# -------------------------------------------\n",
        "# Load data from a website via Llamaindex Loader\n",
        "#\n",
        "# -------------------------------------------\n",
        "\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from IPython.display import Markdown, display\n",
        "import os\n",
        "\n",
        "url = 'https://cinnamon.is/en/news/cinnamon-ai-artificial-intelligence-startup-issues-shares-to-dai-ichi-life-insurance-through-third-party-allotment/'\n",
        "docs = SimpleWebPageReader(html_to_text=True).load_data(\n",
        "    [url]\n",
        ")\n",
        "\n",
        "# --------------------------------------------\n",
        "# Chunking and create embeddings\n",
        "# Automatic via llama index VectorStoreIndex\n",
        "# --------------------------------------------\n",
        "from torch import cuda\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "def load_embedding_model(\n",
        "    model_name: str = \"BAAI/bge-large-en-v1.5\",\n",
        "    device: str = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        ") -> HuggingFaceBgeEmbeddings:\n",
        "    model_kwargs = {\"device\": device}\n",
        "    encode_kwargs = {\n",
        "        \"normalize_embeddings\": True\n",
        "    }  # set True to compute cosine similarity\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs,\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "# setting up the embedding model\n",
        "lc_embedding_model = load_embedding_model()\n",
        "embed_model = LangchainEmbedding(lc_embedding_model)\n",
        "\n",
        "# ### Sanity check embedding model\n",
        "# embedding = lc_embedding_model.embed_query('Hello, world')\n",
        "# embedding = embedding[:10]\n",
        "# print(f'Embedding: {embedding}')\n",
        "\n",
        "# ====== Create vector store and upload data ======\n",
        "Settings.embed_model = embed_model\n",
        "index = VectorStoreIndex.from_documents(docs, show_progress=True)\n",
        "# TODO try async index creation for faster emebdding generation & persist it to memory!\n",
        "# index = VectorStoreIndex(docs, use_async=True)\n",
        "\n",
        "# --------------------------\n",
        "# Set up search query engine\n",
        "# --------------------------\n",
        "\n",
        "# setting up the llm\n",
        "google_api_key = 'AIzaSyDkjiYKPW2P2knSwHelfHsQBrn46n0RHSg'\n",
        "llm = Gemini(model_name=\"models/gemini-pro\", api_key=google_api_key)\n",
        "\n",
        "# # Sanity check llm\n",
        "# resp = llm.complete(\"Hello, world\")\n",
        "# print(resp)\n",
        "\n",
        "# ====== Setup a query engine ======\n",
        "Settings.llm = llm\n",
        "query_engine = index.as_query_engine(similarity_top_k=4)\n",
        "\n",
        "# ---------------------------------------\n",
        "# Customise prompt template + augmenting\n",
        "# ---------------------------------------\n",
        "\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "qa_prompt_tmpl_str = (\n",
        "  \"You are a friendly and supportive assistant for question answering information from website\"\n",
        "  \"Answer the question using the following information delimited by triple brackque, incase case you don't know the answer say 'I don't know!':\\n\\n\"\n",
        "  \"```\\n{context_str}\\n```\"\n",
        "  \"Query: {query_str}\\n\"\n",
        "  \"\\nDon't say based on information provided or something like that\"\n",
        ")\n",
        "\n",
        "qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNutYfasKgcp"
      },
      "outputs": [],
      "source": [
        "# ---------- Chatting -----------\n",
        "from IPython.display import Markdown, display\n",
        "response = query_engine.query('how much money in funds from The Dai-ichi Life Insurance Company')\n",
        "display(Markdown(str(response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtP64HZYOXyG"
      },
      "source": [
        "# GUI with Streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwm1IsD3zMkY",
        "outputId": "701fc7bf-f545-45b5-ca70-179a5e70b29b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.8/72.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for spider-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q langchain-community\n",
        "!pip install -q llama-index\n",
        "\n",
        "!pip install -q llama-index-embeddings-langchain\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "!pip install -q llama-index-llms-gemini\n",
        "!pip install -q google-generativeai\n",
        "!pip install -q llama-index-readers-web\n",
        "\n",
        "!pip install -q streamlit\n",
        "!npm install -q localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTJRvp0KOeUU",
        "outputId": "2da9102d-d901-42d4-e59c-c83f8535617e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import gc\n",
        "\n",
        "import re # website url validation\n",
        "import uuid # unique id for each session\n",
        "import nest_asyncio # allows nested access to the event loop\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import streamlit as st\n",
        "from torch import cuda\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv() # Load Gemini API\n",
        "\n",
        "\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.readers.web import SimpleWebPageReader\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "from llama_index.llms.gemini import Gemini\n",
        "\n",
        "\n",
        "# ---------- Init + Helper function ----------------\n",
        "\n",
        "# os.environ['HF_HOME'] = '\\lit-chat_with_code_RAG\\weights' # for run embedding model locally\n",
        "\n",
        "# setting up the embedding model\n",
        "def load_embedding_model(\n",
        "    model_name: str = \"BAAI/bge-large-en-v1.5\",\n",
        "    device: str = \"cuda\" if cuda.is_available() else \"cpu\"\n",
        ") -> HuggingFaceBgeEmbeddings:\n",
        "    model_kwargs = {\"device\": device}\n",
        "    encode_kwargs = {\n",
        "        \"normalize_embeddings\": True\n",
        "    }  # set True to compute cosine similarity\n",
        "    embedding_model = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs,\n",
        "    )\n",
        "    return embedding_model\n",
        "\n",
        "lc_embedding_model = load_embedding_model()\n",
        "embed_model = LangchainEmbedding(lc_embedding_model)\n",
        "\n",
        "# setting up session\n",
        "if \"id\" not in st.session_state:\n",
        "    st.session_state.id = uuid.uuid4()\n",
        "    st.session_state.file_cache = {}\n",
        "\n",
        "session_id = st.session_state.id\n",
        "client = None\n",
        "\n",
        "# setting up the llm\n",
        "from google.colab import userdata\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "llm = Gemini(model_name=\"models/gemini-pro\", api_key=google_api_key)\n",
        "\n",
        "# helper func\n",
        "def reset_chat():\n",
        "    st.session_state.messages = []\n",
        "    st.session_state.context = None\n",
        "    gc.collect() # free up memory\n",
        "\n",
        "def validate_website_url(url):\n",
        "\n",
        "    url_pattern = re.compile(\n",
        "        r'http[s]?://'  # http:// or https://\n",
        "        r'(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|'  # domain...\n",
        "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+'  # ...or percent-encoded characters\n",
        "        r'(?:\\:[0-9]{1,5})?'  # optional port number\n",
        "        r'(?:/[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),=%]*)*'  # path\n",
        "        r'(?:\\?[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),=%]*)?'  # query string\n",
        "        r'(?:#[a-zA-Z0-9$-_@.&+!*\\\\(\\\\),=%]*)?'  # fragment\n",
        "    )\n",
        "    return bool(url_pattern.match(url))\n",
        "\n",
        "# ---------- End helper function ----------------\n",
        "\n",
        "with st.sidebar:\n",
        "    # Input for URL\n",
        "    website_url = st.text_input(\"URL\")\n",
        "\n",
        "    # Button to load and process url\n",
        "    load_button = st.button(\"Load\")\n",
        "\n",
        "    message_container = st.empty()  # Placeholder for dynamic messages\n",
        "\n",
        "    if load_button and website_url:\n",
        "        if validate_website_url(website_url):\n",
        "            with st.spinner(f\"Loading website...\"):\n",
        "                try:\n",
        "                    # -------------------------------------------\n",
        "                    # Load data from a website via Llamaindex Loader\n",
        "                    # -------------------------------------------\n",
        "                    loader = SimpleWebPageReader()\n",
        "                    docs = loader.load_data([website_url])\n",
        "\n",
        "                    # ---- Create vector store and upload data ---\n",
        "                    # Chunking and create embeddings\n",
        "                    # Automatic via llama index VectorStoreIndex\n",
        "                    # --------------------------------------------\n",
        "                    Settings.embed_model = embed_model\n",
        "                    index = VectorStoreIndex.from_documents(docs)\n",
        "\n",
        "                    # ====== Setup a query engine ======\n",
        "                    Settings.llm = llm\n",
        "                    query_engine = index.as_query_engine(streaming=True, similarity_top_k=4)\n",
        "\n",
        "                    # ====== Customise prompt template ======\n",
        "                    qa_prompt_tmpl_str = (\n",
        "                      \"You are a friendly and supportive assistant.\"\n",
        "                      \"Context information is below.\\n\"\n",
        "                      \"---------------------\\n\"\n",
        "                      \"{context_str}\\n\"\n",
        "                      \"---------------------\\n\"\n",
        "                      \"Try to give the answer that best matches the context information above. In case you don't know the answer, say 'I don't know!'\"\n",
        "                      \"Query: {query_str}\\n\"\n",
        "                      \"Answer: \"\n",
        "                    )\n",
        "                    qa_prompt_tmpl = PromptTemplate(qa_prompt_tmpl_str)\n",
        "\n",
        "                    query_engine.update_prompts(\n",
        "                        {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
        "                    )\n",
        "                    # ======= Complete setting up !!!! ========\n",
        "                    if docs:\n",
        "                        message_container.success(\"Data loaded successfully!!\")\n",
        "                    else:\n",
        "                        message_container.write(\n",
        "                            \"No data found, check if the repository is not empty!\"\n",
        "                        )\n",
        "                    st.session_state.query_engine = query_engine\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"An error occurred: {e}\")\n",
        "                    st.stop()\n",
        "\n",
        "                st.success(\"Ready to Chat!\")\n",
        "        else:\n",
        "            st.error('Invalid url')\n",
        "            st.stop()\n",
        "\n",
        "col1, col2 = st.columns([6, 1])\n",
        "\n",
        "with col1:\n",
        "    st.header(f\"Chat with any website\")\n",
        "\n",
        "with col2:\n",
        "    st.button(\"Clear ↺\", on_click=reset_chat)\n",
        "\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    reset_chat()\n",
        "\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "\n",
        "# Accept user input\n",
        "if prompt := st.chat_input(\"What's up?\"):\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    # Display user message in chat message container\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        message_placeholder = st.empty()\n",
        "        full_response = \"\"\n",
        "\n",
        "        query_engine = st.session_state.query_engine\n",
        "\n",
        "        # Simulate stream of response with milliseconds delay\n",
        "        streaming_response = query_engine.query(prompt)\n",
        "\n",
        "        for chunk in streaming_response.response_gen:\n",
        "            full_response += chunk\n",
        "            message_placeholder.markdown(full_response + \"▌\")\n",
        "\n",
        "        message_placeholder.markdown(full_response)\n",
        "\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSpquYwROi7V",
        "outputId": "460146c4-a5bb-44cc-be64-ad120a56ea2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.90.31.54\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.948s\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "CVEqUqJcKPdH"
      ],
      "authorship_tag": "ABX9TyPgmSct5zjas8twgX477qSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07999175b7db48d6a9a5ae59b651428b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ac65e4a4b3348dcbef179ad494f4927",
            "placeholder": "​",
            "style": "IPY_MODEL_a39a3b3f845b4267982f56e6b8e87298",
            "value": " 2/2 [00:00&lt;00:00,  6.07it/s]"
          }
        },
        "14f2fc9deb5f412a88fa7694bace7752": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15d0e6881e8a4134b61ddcfdb3cdd545": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dedae21d4fc4be19c461dc4ffad53b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27f17b36e8704cd0b9c6a3965c93af14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15d0e6881e8a4134b61ddcfdb3cdd545",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dedae21d4fc4be19c461dc4ffad53b2",
            "value": 2
          }
        },
        "4124c7030d9448f6a2f0f312c717d92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1d02d0b25b4818ba1c7e0860533c86",
            "placeholder": "​",
            "style": "IPY_MODEL_4cecaec3b11c4915a5a0ca8a0092e1e0",
            "value": "Parsing nodes: 100%"
          }
        },
        "416aa61e14014266aef39637e8182c78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "460ec774bda54dd29da57bf313c877e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4124c7030d9448f6a2f0f312c717d92a",
              "IPY_MODEL_af925b0ca8e84713aa94532af835b69e",
              "IPY_MODEL_641ec30a759f4ccfa6f6834d129b6c4e"
            ],
            "layout": "IPY_MODEL_80b9c3f5a0134c37b15664cf93d603a3"
          }
        },
        "4cecaec3b11c4915a5a0ca8a0092e1e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d9e20e7c0d94fc4aafc7c385dcedf85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641ec30a759f4ccfa6f6834d129b6c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_416aa61e14014266aef39637e8182c78",
            "placeholder": "​",
            "style": "IPY_MODEL_14f2fc9deb5f412a88fa7694bace7752",
            "value": " 1/1 [00:00&lt;00:00, 14.49it/s]"
          }
        },
        "6b1d02d0b25b4818ba1c7e0860533c86": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7978afa8c91b462d913329881e35a2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80b9c3f5a0134c37b15664cf93d603a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9244cbd583c049c8a1ccad35fe31186d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9252b45e0ffd472a9df77218e2a3b00b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db5960481a92496a9f384ea565815965",
              "IPY_MODEL_27f17b36e8704cd0b9c6a3965c93af14",
              "IPY_MODEL_07999175b7db48d6a9a5ae59b651428b"
            ],
            "layout": "IPY_MODEL_afdd8058a1134c0eba7dd218a60ac623"
          }
        },
        "9467733a47a947bfbff60575465e3706": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9ac65e4a4b3348dcbef179ad494f4927": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a39a3b3f845b4267982f56e6b8e87298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af925b0ca8e84713aa94532af835b69e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9244cbd583c049c8a1ccad35fe31186d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9467733a47a947bfbff60575465e3706",
            "value": 1
          }
        },
        "afdd8058a1134c0eba7dd218a60ac623": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db5960481a92496a9f384ea565815965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d9e20e7c0d94fc4aafc7c385dcedf85",
            "placeholder": "​",
            "style": "IPY_MODEL_7978afa8c91b462d913329881e35a2a7",
            "value": "Generating embeddings: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}